{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "397d56f4-15d5-4daf-86df-77dae28e0966",
   "metadata": {},
   "source": [
    "Q1. What is an activation function in the context of artificial neural networks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5910babc-6179-4e07-bd01-5eb622ab244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it. \n",
    "## The purpose of the activation function is to introduce non-linearity into the output of a neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf95b8e0-3951-475b-a3e3-ca43fb71a50b",
   "metadata": {},
   "source": [
    "Q2. What are some common types of activation functions used in neural networks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b13d8664-a1b2-480c-bbe0-309eac757879",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some common Activatio Functions are\n",
    "## 1. Sigmoid \n",
    "## 2. Softmax\n",
    "## 3. Relu\n",
    "## 4. TanH\n",
    "## 5. Linear\n",
    "## 6. Leaky Relu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c154603f-3f65-435a-b2f2-5a2cea9dbcce",
   "metadata": {},
   "source": [
    "Q3. How do activation functions affect the training process and performance of a neural network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e756048d-6320-4f64-b3ff-fc161fd81c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Activation Function introduce non-linearities into the network, allowing it to model complex relationships between inputs and outputs.\n",
    "## Without non-linear activation functions, the network would be limited to representing only linear relationships.\n",
    "## During the training process, neural networks use gradient-based optimization algorithms, such as backpropagation, to adjust the weights and biases\n",
    "## Activation functions also determine the output range of neurons. Some activation functions, like sigmoid or tanh, squash the output between 0 and 1 or -1 and 1, respectively.\n",
    "## This property can be beneficial for tasks such as binary classification. Other activation functions, like linear or softmax, provide different output ranges suitable for\n",
    "## specific problem domains, such as regression or multiclass classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a41f66-0338-4652-a353-9dd2a34497f2",
   "metadata": {},
   "source": [
    "Q4. How does the sigmoid activation function work? What are its advantages and disadvantages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74501fd0-f651-4281-acc7-d9a4da3a976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The sigmoid function takes the weighted sum of inputs and biases (net input) and applies the sigmoid transformation to it. It squashes the net input into a range between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a522c9bd-670f-4908-927d-7331c134c0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sigmoid Function is used when output is binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbe6b6e1-2c73-469d-bab1-254797e159a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The sigmoid function is defined as:\n",
    "\n",
    "#    f(x) = 1 / (1 + e^(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c339fc7c-8f7a-45b9-b7c7-76a7ea0305a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Advantages \n",
    "## sigmoid maps the input value to range between 0 and 1 , making it suitable for binary classification problem\n",
    "## The sigmoid function introduces non-linearity, allowing neural networks to model complex relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1cf4018-0b8c-4be7-b39e-67e9f9b81254",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Disadvantages\n",
    "## It suffers from vanishing gradient problem.\n",
    "## AS we do backpropagate and calculate derivatives where the derivative ranges between 0 and .25 , and chain rule is applied then it becomes very small value so it does not able to learn \n",
    "## The sigmoid function is not zero-centered, meaning its outputs are always positive. This property can make the optimization process slower in neural networks, as the updates to weights depend\n",
    "## on the gradients, which can be positive or negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6436bf2-c631-446f-a3a3-f6a38007a59b",
   "metadata": {},
   "source": [
    "Q5.What is the rectified linear unit (ReLU) activation function? How does it differ from the sigmoid function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96617856-7fc2-45fa-8500-fabb0fc2b05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ReLU : if the input value is positive then produces same positive values but if the input is negative then produces zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "084745fe-f1c6-4bc6-9934-f59b7a15387b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ReLU Function: \n",
    "## f(x) = max(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9aa8ff3d-e123-4a86-910e-024168fc1565",
   "metadata": {},
   "outputs": [],
   "source": [
    "## It differs from sigmoid \n",
    "## in sigmoid it squashes value in range 0 and 1, means max is 1 while in ReLu value for positive can be same postive value that can go any value above 1\n",
    "## Sigmoid suffers from vanishing gradient problem In contrast, ReLU addresses the vanishing gradient problem by providing a constant gradient of 1 for positive inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9ab37b-d626-4dbb-810d-6f06d421705c",
   "metadata": {},
   "source": [
    "Q6. What are the benefits of using the ReLU activation function over the sigmoid function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a253a74c-9a9c-4f83-b936-8891b80d5ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Advatges of using RelU over sigmoid\n",
    "## It address the problem of vanishing gradient which occurs in sigmoid activation function\n",
    "## Faster convergence and training speed: ReLU has a computationally efficient implementation, as it involves simple thresholding operations.\n",
    "## Linear behavior and large-scale linear representations: ReLU behaves linearly for positive inputs, which makes it more suitable for capturing large-scale linear relationships in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c549a3e9-a55a-466a-84cf-c43c1b1f8162",
   "metadata": {},
   "source": [
    "Q7. Explain the concept of \"leaky ReLU\" and how it addresses the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d36810c8-e613-4335-9eca-2dbce8b0a2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Leaky Rectified Linear Unit, or Leaky ReLU, is a type of activation function based on a ReLU, but it has a small slope for negative values instead of a flat slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26596962-4120-44a3-9f04-9039b5f79be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Leaky ReLU:\n",
    "## f(x): { x, x>0\n",
    "##        A*x, x<=0}\n",
    "## A is some constant  typically in the range of 0.01 to 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1faa8bb7-1916-4be9-9315-0b1e380ad879",
   "metadata": {},
   "outputs": [],
   "source": [
    "## By allowing a non-zero slope for negative inputs, leaky ReLU provides a gradient flow even for negative values, which can help alleviate the vanishing gradient problem.\n",
    "## The non-zero slope ensures that gradients do not vanish completely, enabling the network to learn more effectively, especially in deeper architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d2a9ec-5273-4c6c-bd38-f65800d1551b",
   "metadata": {},
   "source": [
    "Q8. What is the purpose of the softmax activation function? When is it commonly used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39661947-4a27-464d-aee1-7cab6f811313",
   "metadata": {},
   "outputs": [],
   "source": [
    "## softmax is used as the activation function for multi-class classification problems where class membership is required on more than two class labels\n",
    "## The softmax function is used as the activation function in the output layer of neural network models that predict a multinomial probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909b4a3e-5d41-40bc-914d-0978babeb624",
   "metadata": {},
   "source": [
    "Q9. What is the hyperbolic tangent (tanh) activation function? How does it compare to the sigmoid function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d20aa85-c7ca-422b-b76c-a00402a08817",
   "metadata": {},
   "outputs": [],
   "source": [
    "## It is a variant of the sigmoid function and has a similar S-shaped curve. The tanh function maps the input to a range between -1 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99442161-7ae3-41e7-a2f4-f700bcd50298",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tanh Function\n",
    "## f(x)=(e^x - e^(-x)) / (e^x + e^(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9a820a0-8244-49ea-8298-8a38273f7bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The sigmoid function maps the input to a range between 0 and 1, while the tanh function maps the input to a range between -1 and 1.\n",
    "## The tanh function produces negative outputs for negative inputs and positive outputs for positive inputs, making it symmetric around the origin\n",
    "## Both the sigmoid and tanh functions introduce non-linearity to the network. However, the tanh function is steeper around the origin compared to the sigmoid function, \n",
    "## which means it exhibits stronger non-linear behavior. \n",
    "##  Unlike the sigmoid function, the tanh function is zero-centered, meaning its outputs are centered around zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dc2878-eba8-402d-9e6f-b97a6c14006f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
